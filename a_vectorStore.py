"""

 When it comes to rag , when we have to store vectors in data base then three major problems arise.
 First is that sometime given dataset is in large quantity and we have to store it in our system so it may not be possible for our system to store this data.
 Second problem is that we have to generate embedings of large data for storing it in our system.So for generating embeddings of large data is a very complex task.
 Third one arises when it comes to sematic search from these vecotrs.When our system has to fetch only related content from data.
 
 So to overcome these problems vector store help us.
A vector store is a system designed to store and retrieve data represented as numerical vectors.
key features:
    i-Storage : it can store data in-memory(RAM) for quick lookups or on-disk for durability and large-scale use.
    
    ii-Similarity Search : Helps retrieve the vectors most similar to a query vecotr.
    
    iii-Indexing : Provide a data structure or method that enables fast similarity searches on high-demand vectors
    
    iv-CRUD Operations - Manage the lifecycle of data - adding new vectors,reading them,updating existing entries,removing outdated vectors.
    
Use Cases :-
    1.Semantic Search
    2-RAG
    3-Recommender Systems
    4-Image/Multimedia search.
    

Vector Store vs Vector Database
-------------------------------
Vector Store refers to a lightweight library or service that focuses on storing vectors and performing similarity search.
May not include many traditional database features like transactions,rich query language or role base acess control.
Ideal for prototyping,smaller-scale applications

Vector database is a vector store with extra database features(e.g. clustering , scaling , security , metadata filtering and durability)


"""

# We have write code for vector stores in Google Colab


from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

doc1 = Document(page_content="ðŸ‡µðŸ‡° Babar Azam â€“ Classy, consistent, and Pakistanâ€™s batting maestro.", metadata={"source": "doc1"})
doc2 = Document(page_content="ðŸ‡µðŸ‡° Shaheen Afridi â€“ Fierce pace, deadly swing, and a match-winner.", metadata={"source": "doc2"})
doc3 = Document(page_content="ðŸ‡®ðŸ‡³ Virat Kohli â€“ Modern-day legend with unmatched intensity and records.", metadata={"source": "doc3"})
doc4 = Document(page_content="ðŸ‡µðŸ‡° Shoaib Malik â€“ Veteran all-rounder, calm under pressure, evergreen performer.", metadata={"source": "doc4"})
doc5 = Document(page_content="ðŸ‡¿ðŸ‡¦ Rilee Rossouw â€“ Explosive left-hander, known for big hits and quick runs.", metadata={"source": "doc5"})

docs = [doc1 , doc2 , doc3 , doc4 , doc5]

llm = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    
)

# for creating a vector store with the help of chroma we have to pass three arguments in chroma one is embedding_function in which we have to pass our embedding model , second one is name of directory in which we want to creater our data and third one is name of collection(table)
chroma_Store = Chroma(
    embedding_function=llm,
    persist_directory="my_chroma_db",
    collection_name="my_collection",
)

# for inserting docs in our store we use add_documents method of store and it will return id of each document after inserting them
chroma_Store.add_documents(docs)

# for getting data from store we use get method and in its include argument we have to pass a list of things that we want to get from store

chroma_Store.get(include=["embeddings"])

# if we want to get some data on the basis of any query than we can use similarity_search method of store inside which we pass two args one is query and other is number of results that we want to get
chroma_Store.similarity_search("tell me about player who is good in situation of pressure?" , k=1 , )


# we can also check score of result generated by store using a little different method -> similarity_search_with_score  and score greater than or equal to 1 is bad.Basically it is the value of difference between query and result.
chroma_Store.similarity_search_with_score("tell me about player who is good in situation of pressure?" , k=1  )

# we can also update any document using update_document and in it we pass document_id and new document 
# updated_doc = Document(
#     page_content="content",
#     metadata = {'source':'something'}
# )
# chroma_Store.update_document(
#     document_id="id" , document=updated_doc
# )

# we can also delete any document from store by using delete method in this method we pass list of ids of those docs to which we want to delete.
chroma_Store.delete(ids=[])