# RunnableBranch -> It is control flow component in Langchain that allows you to conitionally route input data to different cahns or runnables based on custom logic.
# It functions like an if/elif/else block fo rchains - where you define a set of condition functions , each associated with a runnable(e.g. LLM call , prompt chain or tool).The first matching condition is executed.If no condition matches a default runnable is used(if provided)

# here we are going to do an activity.We will be giving a prompt to llm ,if response generated by llm is of length of more than 300 then we will ask him to summarize it else we will be printing it as it is


from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate                   
from langchain_core.output_parsers import StrOutputParser
from langchain.schema.runnable import RunnableSequence , RunnableBranch,RunnableLambda , RunnablePassthrough
from dotenv import load_dotenv
import os

load_dotenv()

gemini_api_key=os.getenv("GEMINI_API_KEY")

model = ChatGoogleGenerativeAI(
    model = 'gemini-2.0-flash',
    google_api_key = gemini_api_key
)

template1 = PromptTemplate(
    template="""
        Generate a detiled report on given topic and at the end write a word generated. topic-> {topic}
    """,
    input_variables=['topic']
)

parser = StrOutputParser()

template2 = PromptTemplate(
    template="""
    Summarize the given report -> {report}
    """,
    input_variables=['report']
)
def word_count(text):
    return len(text.split())>300
report = RunnableSequence(template1,model , parser)

response = RunnableBranch(
    # here in RunnableBranch we pass tuples and each tuple we pass two arguments,one is our condition and second one is a chain or something that we want to execute.We can pass as many tuples as many we want on the basis of required conditions.And at last we can pass a default chain that will run if any condition above does not run
    (RunnableLambda(word_count) , RunnableSequence(template2 , model , parser)),
    RunnablePassthrough()
)

chain = RunnableSequence(report,response)

topic = input("Enter a topic name : ")
print(chain.invoke({'topic' : topic}))
